{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50eb0a68-a1d2-4361-8403-87d0f34d921b",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "*******************************************************************************************\n",
    "    \n",
    "### Motion Tracking in Video Files with ALphaPose\n",
    "\n",
    "\n",
    "##### 29 January 2025\n",
    "\n",
    "##### Juan Ignacio Mendoza Garay  \n",
    "\n",
    "*******************************************************************************************\n",
    "\n",
    "</center>\n",
    "\n",
    "##### INFORMATION:\n",
    "\n",
    "* Extract one point of the body (in this demonstration the nose), for every tracked person in the picture. Then, get rid of extraneous data, interpolate missing data, and rearrange the tabular data. The resulting table has columns {p1_x,p1_y,p2_x,p2_y,...} where p is a tracked person from left to right, and {x,y} are horizontal and vertical coordinates of the point.\n",
    "\n",
    "* Tested using:\n",
    "\n",
    "    * AlphaPose\n",
    "        * Fork: https://github.com/juigmend/Alpha_Pose\n",
    "    * Python 3.11\n",
    "    * Windows 11 operating system\n",
    "    * Intel 64-bit CPU\n",
    ">\n",
    "* Dependencies:\n",
    "\n",
    "    * Youtube video downloader: https://pypi.org/project/yt-dlp/\n",
    "    * cython_bbox:\n",
    "        1) install Desktop Development with C++ from the Visual Studio Installer\n",
    "        2) type to command prompt: \\\n",
    "           set DISTUTILS_USE_SDK=1 \n",
    "        3) install cython_bbox (e.g., using pip, conda, or other method)\n",
    "    * Other packages might be prompted for installation.\n",
    ">\n",
    "* Instructions:\n",
    "\n",
    "    Edit the values indicated with an arrow like this: <---  \n",
    "    Comment/uncomment or change values as suggested by the comments.  \n",
    "    Run the program, close your eyes and hope for the best.  \n",
    "\n",
    "*******************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ecbf0370-48b4-4ba3-8c67-7f916ddbd6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import exists, basename, splitext, isfile, isdir, join\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import io\n",
    "import base64\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ef3bf-644d-4167-ab58-d426e1c01de9",
   "metadata": {},
   "source": [
    "***\n",
    "#### Set paths and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2c0ddae5-0936-4995-b187-ded904095801",
   "metadata": {},
   "outputs": [],
   "source": [
    "AP_code_path = r\"C:\\Users\\Gabriel_Rorke\\Ghosts\\Python\\AlphaPose\" # <--- folder of AlphaPose code\n",
    "\n",
    "video_in_folder =  r\"C:\\Users\\Gabriel_Rorke\\Ghosts\\Borodin_Quartet\\input_video\" \n",
    "video_in_path = video_in_folder # <--- path for input video file or folder with input video files\n",
    "\n",
    "logs_path = r\"C:\\Users\\Gabriel_Rorke\\Ghosts\\Borodin_Quartet\\pose_tracking_results\" # <--- folder for log files\n",
    "video_out_path = logs_path+r'\\video' # <--- folder for resulting AlphaPose video files (None to not save video)\n",
    "\n",
    "json_path = logs_path+r'\\tracking'        # <--- folder for resulting AlphaPose tracking files\n",
    "figures_path = logs_path+r'\\figures'      # <--- folder for data inspection files\n",
    "preproc_path = logs_path+r'\\preprocessed' # <--- folder for preprocessed data  files\n",
    "\n",
    "fps = 29.97   # <--- fps (frames per second of input video)\n",
    "n_persons = 4 # <--- expected number of individuals to be tracked\n",
    "series_selection = [0,1] # <--- x and y of one point ([x1,y1] for \"Nose\", assuming COCO format)\n",
    "\n",
    "overwrite_results = False # <--- recompute tracking (False will skip already processed videos)\n",
    "\n",
    "save_parquet = True # <--- save pre-processed data to parquet file\n",
    "save_raw_fig = 'concealed' # <--- save raw data figure: True, False or 'concealed'\n",
    "save_pp_fig = 'concealed'  # <--- save preprocesed data figure: True, False or 'concealed'\n",
    "save_AP_log = True  # <--- save pose tracking log\n",
    "save_pp_log = True  # <--- save peprocessing log\n",
    "verbose = False     # <--- display information and warnings\n",
    "\n",
    "t_range = 'all'  # <--- list of time range to plot in frames (fps*seconds = frames) or 'all'\n",
    "markersize = 0.8 # <--- marker size for plots\n",
    "linewidth = 2    # <--- line width for plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2874fd-5b88-45ae-b7e5-771a9f46c768",
   "metadata": {},
   "source": [
    "***\n",
    "#### Pre-trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ef96200a-7c3a-4b0b-af69-2a4ff00f04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most probably there is no need to alter the following:\n",
    "yolo_pretrained_model_path = AP_code_path + r'\\detector\\yolo\\data\\yolov3-spp.weights'\n",
    "pretrained_model_path = AP_code_path + r'\\pretrained_models\\fast_421_res152_256x192.pth'\n",
    "pretrained_model_config_path = AP_code_path + r'\\configs\\coco\\resnet\\256x192_res152_lr1e-3_1x-duc.yaml'\n",
    "tracker_weights_path = AP_code_path + r'\\trackers\\weights\\osnet_ain_x1_0_msmt17_256x128_amsgrad_ep50_lr0.0015_coslr_b64_fb10_softmax_labsmth_flip_jitter.pth'\n",
    "\n",
    "# OBJECT DETECTION:\n",
    "if not exists(yolo_pretrained_model_path):\n",
    "    ! mkdir {AP_code_path}\\detector\\yolo\\data\n",
    "    ! gdown -O {yolo_pretrained_model_path} https://drive.google.com/uc?id=1D47msNOOiJKvPOXlnpyzdKA3k6E97NTC\n",
    "\n",
    "# POSE DETECTION:\n",
    "if not exists(pretrained_model_path):\n",
    "    ! gdown -O {pretrained_model_path} https://drive.google.com/uc?id=1kfyedqyn8exjbbNmYq8XGd2EooQjPtF9 # Fast Pose (DUC)\n",
    "# Documentation: https://github.com/MVIG-SJTU/AlphaPose/blob/master/docs/MODEL_ZOO.md\n",
    "\n",
    "# POSE TRACKING:\n",
    "if not exists(tracker_weights_path):\n",
    "    ! mkdir {AP_code_path}\\trackers\\weights\n",
    "    ! gdown -O {tracker_weights_path} https://drive.google.com/uc?id=1myNKfr2cXqiHZVXaaG8ZAq_U2UpeOLfG # Human-ReID\n",
    "# Documentation: https://github.com/MVIG-SJTU/AlphaPose/tree/master/trackers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1124ac-3c44-4c34-acde-cf789f149959",
   "metadata": {},
   "source": [
    "***\n",
    "#### Get filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c4378e7-13ea-43b0-b78a-11663bc3d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isdir(video_in_path):\n",
    "    ffn_lst = []\n",
    "    fn_lst = []\n",
    "    for fn in listdir(video_in_path):\n",
    "        ffn = join(video_in_path, fn)\n",
    "        if isfile(ffn):\n",
    "            ffn_lst.append( ffn ) \n",
    "            fn_lst.append( fn ) \n",
    "            \n",
    "elif isfile(video_in_path): \n",
    "    ffn_lst = [video_in_path]\n",
    "    fn_lst = [splitext(basename(video_in_path))[0]]\n",
    "\n",
    "n_files = len(ffn_lst)\n",
    "\n",
    "if not overwrite_results: json_saved_fn = listdir(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd60306-15b4-41c6-af0d-c91ccbc02576",
   "metadata": {},
   "source": [
    "***\n",
    "#### Run AlphaPose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8d3161e7-d670-48be-865f-b03e20a1efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_video_str = ''\n",
    "if video_out_path:\n",
    "    save_video_str = f'--visoutdir {video_out_path} --save_video '\n",
    "\n",
    "for ffn, fn in zip(ffn_lst,fn_lst):\n",
    "\n",
    "    fn_ne = fn.split('.')[0]\n",
    "    \n",
    "    new_file = True\n",
    "    if not overwrite_results:\n",
    "        json_fn = f'alphapose-results_{fn_ne}.json'\n",
    "        new_file = json_fn not in json_saved_fn\n",
    "\n",
    "    if overwrite_results or new_file:\n",
    "\n",
    "        if verbose: print(f'AlphaPose - processing {fn}')\n",
    "        \n",
    "        if save_AP_log: \n",
    "            AP_log_txt = [fn + '\\n']\n",
    "            tic = time.time()\n",
    "\n",
    "        # AlphaPose:\n",
    "        ! cd {AP_code_path} && python scripts\\demo_inference.py --sp --video {ffn} \\\n",
    "        --jsonoutdir {json_path} {save_video_str} --checkpoint {pretrained_model_path} \\\n",
    "        --cfg {pretrained_model_config_path} --pose_track --suffix {fn_ne}\n",
    "    \n",
    "        # Save pose tracking log:\n",
    "        if save_AP_log: \n",
    "            AP_log_txt.append(f\"toc = {timedelta(seconds = time.time() - tic)} (H:M:S)\\n\")\n",
    "            txtlog_ffn = logs_path + '\\\\' + 'AP_log.txt'\n",
    "            AP_log_txt.append('\\n')\n",
    "            with open(txtlog_ffn, 'a') as output:\n",
    "                for t in AP_log_txt:\n",
    "                    output.write(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42d49b-5a5e-4b90-bd11-e99b7d89560b",
   "metadata": {},
   "source": [
    "***\n",
    "#### Pre-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a1c6ebe-e034-497c-9ba3-a188e0e4a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_labels = ['x','y']\n",
    "\n",
    "if not overwrite_results: json_saved_fn = listdir(json_path)\n",
    "    \n",
    "for fn in fn_lst:\n",
    "\n",
    "    fn_ne = fn.split('.')[0]\n",
    "    \n",
    "    new_file = True\n",
    "    if not overwrite_results:\n",
    "        json_fn = f'alphapose-results_{fn_ne}.json'\n",
    "        new_file = json_fn not in json_saved_fn\n",
    "\n",
    "    if overwrite_results or new_file:\n",
    "    \n",
    "        # Load data from JSON file produced by AlphaPose:\n",
    "        json_fn = f'alphapose-results_{fn_ne}.json'\n",
    "        data_raw_df = pd.read_json(json_path + '\\\\' + json_fn)\n",
    "    \n",
    "        # Reduce by removing unnecessary data:\n",
    "        if len(series_selection) > 2:\n",
    "            raise Exception(''.join(['only one point with two dimensions (x,y) allowed, ',\n",
    "                                     f'but instead got this: {series_selection}']))\n",
    "        data_red_df = data_raw_df.drop(['category_id','keypoints','score','box'],axis=1)\n",
    "        for lbl,i in zip(series_labels,series_selection):\n",
    "            data_red_df[lbl] = data_raw_df.keypoints.str[i]\n",
    "        data_red_df.image_id = data_red_df.image_id.str.split('.').str[0].astype(int)\n",
    "    \n",
    "        # Inspect and make plot of raw data:\n",
    "        if save_raw_fig or save_pp_log:\n",
    "            if save_pp_log: pp_log_txt = [fn_ne + '\\n']\n",
    "            if t_range == 'all':\n",
    "                t_range = [0,data_red_df.image_id.max()]\n",
    "            n_series = len(series_selection)\n",
    "            persons_range = range(1,n_persons+1)\n",
    "            for i_s in range(n_series):\n",
    "                if save_raw_fig: plt.subplot(n_series,1,i_s+1)\n",
    "                n_frames = []\n",
    "                legend = []\n",
    "                for i_p in persons_range:\n",
    "                    data_red_slice_df =\\\n",
    "                        data_red_df[series_labels[i_s]][  (data_red_df.idx == i_p) \n",
    "                                                        & (data_red_df.image_id >= t_range[0]) \n",
    "                                                        & (data_red_df.image_id < t_range[1]) ]\n",
    "                    if save_raw_fig: data_red_slice_df.plot(linewidth=linewidth)        \n",
    "                    n_frames.append(len(data_red_slice_df))   \n",
    "                if save_raw_fig:\n",
    "                    data_red_df[series_labels[i_s]].plot( marker='.', linestyle='none', \n",
    "                                                          markersize=markersize, color='k')\n",
    "                    plt.ylabel(series_labels[i_s])\n",
    "                    if i_s == 0: \n",
    "                        plt.legend( list(persons_range)+['all'],loc='upper right', \n",
    "                                    bbox_to_anchor=(1.2, 1.02) )\n",
    "                if save_pp_log: \n",
    "                    mean_persons = sum(n_frames)/n_persons\n",
    "                    for p in n_frames:\n",
    "                        if p != mean_persons:\n",
    "                            warning_frames = ''.join([ 'inconsistent frame count in ,'\n",
    "                                                      f'{series_labels[i_s]} {tuple(n_frames)}' ])\n",
    "                            pp_log_txt.append( warning_frames+'\\n' )\n",
    "                            if verbose: print('Warning:',warning_frames)\n",
    "                            break\n",
    "            if save_raw_fig:\n",
    "                plt.gcf().suptitle(fn_ne+'\\nRaw Data')\n",
    "                plt.gcf().supxlabel('stacked frames (as in json file)')\n",
    "                plt.tight_layout()\n",
    "                fig_ffn = figures_path + '\\\\' + fn_ne + '_RAW.png'\n",
    "                plt.savefig(fig_ffn)  \n",
    "            if save_raw_fig == 'concealed': plt.close(plt.gcf())\n",
    "            if save_pp_log or verbose:\n",
    "                if (data_red_df.idx.max()) != n_persons:\n",
    "                        warning_idx = 'more idx than number of people in raw data'\n",
    "                        if verbose: print('Warning:',warning_idx)\n",
    "                        if save_pp_log: pp_log_txt.append(warning_idx+'\\n')\n",
    "    \n",
    "        # Rearrange such that each row is a frame (image_id), and fill missing data:\n",
    "        data_rar_df = pd.DataFrame( list(range(data_red_df.image_id.max() + 1)) , columns=['image_id'])\n",
    "        for i in persons_range:\n",
    "            data_rar_df = data_rar_df.merge( \n",
    "                data_red_df[['image_id']+series_labels][(data_red_df.idx == i)],\n",
    "                on='image_id', how='left', suffixes=(f'_{i-1}',f'_{i}') )\n",
    "        data_rar_df = data_rar_df.drop(['image_id'],axis=1)\n",
    "        found_nan = data_rar_df.isnull().values.any()\n",
    "        if save_pp_log or verbose:\n",
    "            if found_nan:\n",
    "                data_rar_df = data_rar_df.interpolate(limit_direction='both')\n",
    "                warning_interp = 'missing raw data have been interpolated'\n",
    "                if verbose: print('Warning:',warning_interp)\n",
    "                if save_pp_log: pp_log_txt.append(warning_interp+'\\n')\n",
    "    \n",
    "        # Re-order and re-label columns in order from left to right as they appear in the image:\n",
    "        # It is assumed that the persons don't relocate (e.g. they are sitting or standing in one place).\n",
    "        # Indices are set to start at 0 to be consistent with Python indexing.\n",
    "        new_order_x = [ x for x in data_rar_df.iloc[:,::2].median().sort_values().index]\n",
    "        new_order_y = [ y.replace('x','y') for y in new_order_x ]\n",
    "        new_order_xy = []\n",
    "        new_order_lbl = []\n",
    "        i_c = 0\n",
    "        for x,y in zip(new_order_x,new_order_y):\n",
    "            new_order_xy.append(x)\n",
    "            new_order_xy.append(y)\n",
    "            new_order_lbl.append(f'{i_c}_x')\n",
    "            new_order_lbl.append(f'{i_c}_y')\n",
    "            i_c += 1\n",
    "        data_rar_df = data_rar_df.reindex(new_order_xy, axis=1)\n",
    "        data_rar_df.columns = new_order_lbl\n",
    "    \n",
    "        # Save preprocessing log:\n",
    "        if save_pp_log:\n",
    "            txtlog_ffn = logs_path + '\\\\' + 'pp_log.txt'\n",
    "            pp_log_txt.append('\\n')\n",
    "            with open(txtlog_ffn, 'a') as output:\n",
    "                for t in pp_log_txt:\n",
    "                    output.write(t)\n",
    "    \n",
    "        # Make plot of pre-processed data:\n",
    "        if save_pp_fig:\n",
    "            if t_range == 'all':\n",
    "                t_range = [0,data_rar_df.index.max()]\n",
    "            for i_s in range(n_series):\n",
    "                plt.subplot(n_series,1,i_s+1)\n",
    "                legend = []\n",
    "                names_cols = [ f'{n}_{series_labels[i_s]}' for n in range(n_persons)]\n",
    "                for nc in names_cols:\n",
    "                    data_rar_slice_df = data_rar_df[nc].iloc[ t_range[0] : t_range[1] ]\n",
    "                    data_rar_slice_df.plot(linewidth=linewidth)   \n",
    "                    legend.append(nc.split('_')[0])\n",
    "                    n_frames.append(len(data_red_slice_df))\n",
    "                plt.ylabel(series_labels[i_s])\n",
    "                if i_s == 0: \n",
    "                    plt.legend(legend,loc='upper right', bbox_to_anchor=(1.2, 1.02))\n",
    "            plt.suptitle(fn_ne+'\\nPre-processed Data')\n",
    "            plt.xlabel('time (video frames)')\n",
    "            plt.tight_layout()\n",
    "            fig_ffn = figures_path + '\\\\' + fn_ne + '_PP.png'\n",
    "            plt.savefig(fig_ffn)\n",
    "            if save_pp_fig == 'concealed': plt.close(plt.gcf())\n",
    "    \n",
    "        # Write pre-processed data to a file:\n",
    "        if save_parquet:    \n",
    "            AP_pp_ffn = preproc_path + '\\\\' + fn_ne + '.parquet'\n",
    "            data_rar_df.to_parquet(AP_pp_ffn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
